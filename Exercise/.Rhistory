#hPrice
hprice.dat <- read.table("Dataset/hprice.txt",header=TRUE)
hprice.dat
View(hprice.dat)
summary(hprice.dat)
hprice.lm1 <- lm(sale.price ~ lot.size + desire.loc,data=hprice.dat)
hprice.lm1
summary(hprice.lm1)
summary(hprice.dat)
# first you need to load the MASS package (you can also do this by selecting Load Package from the Packages menu).
library(MASS)
hprice.step <- stepAIC(hprice.lm2,scope=list(lower = ~1,upper = ~.^2),k=log(546))
# give a summary of the search process
hprice.step$anova
hprice.lm2 <- lm(sale.price ~ .,data=hprice.dat)
summary(hprice.lm2)
# first you need to load the MASS package (you can also do this by selecting Load Package from the Packages menu).
library(MASS)
hprice.step <- stepAIC(hprice.lm2,scope=list(lower = ~1,upper = ~.^2),k=log(546))
# give a summary of the search process
hprice.step$anova
summary(hprice.step)
x <- seq(0,1,len=10)
x <- seq(0,1,len=10)
x
t <- sin(2*pi*x)+rnorm(n=10,mean=0,sd=0.3)
t
plot(x,t,ylim=c(-2,2))
x.graph <- seq(0,1,by=0.001)
t.graph <- sin(2*pi*x.graph)
x.graph
t.graph
plot(x,t,ylim=c(-2,2))
lines(x.graph,t.graph,lwd=2,col=4)
install.packages("ridge")
install.packages("ridge", lib="S:/patrec/Rpackages/")
x.graph <- seq(0,1,by=0.001)
t.graph <- sin(2*pi*x.graph)
# plot the training set. You might want to adjust the value of ylim, depending on drawn data points and fitted functions
plot(x,t,ylim=c(-2,2))
# plot the "true" function
lines(x.graph,t.graph,lwd=2,col=4)
lm(t~x+I(x^2)+I(x^3),data.frame(t=t,x=x))
predict(poly.3,data.frame(x=x.graph))
# fit a 3rd order polynomial to the data
poly.3 <- lm(t~x+I(x^2)+I(x^3),data.frame(t=t,x=x))
predict(poly.3,data.frame(x=x.graph))
lines(x.graph,y.poly.3.graph,lwd=2,col=2)
lines(x.graph,y.poly.3.graph,lwd=2,col=2)
y.poly.3.graph <- predict(poly.3,data.frame(x=x.graph))
# plot the fitted function
lines(x.graph,y.poly.3.graph,lwd=2,col=2)
poly.9 <- lm(t~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8)+I(x^9),data.frame(t=t,x=x))
summary(poly.9)
t.testset <- sin(2*pi*x.graph)+rnorm(n=1001,mean=0,sd=0.3)
error.poly.3 <- sum((y.poly.3.graph-t.testset)^2)
error.poly.9 <- sum((y.poly.9.graph-t.testset)^2)
error.poly.3 <- sum((y.poly.3.graph-t.testset)^2)
error.poly.3
y.poly.9.graph <- predict(poly.9,data.frame(x=x.graph))
lines(x.graph,y.poly.9.graph,lwd=2,col=6)
#loading the data
digit.dat <- read.csv("dataset/mnist.csv", header = TRUE, sep = ",")
#preprocessing the data
digit.dat$label = as.factor(digit.dat$label)
#checking stats. the result shows that a lot of pixels remain white for ALL 42000 samples. They can be clearly eliminated.
summary(digit.dat)
inkDensity <- apply(digit.dat[,-1],1,sum)
inkMeanStats <- tapply(inkDensity,digit.dat$label,mean)
inkSdStats <- tapply(inkDensity,digit.dat$label,sd)
#scale features
ScaledinkSdStats <- scale(inkSdStats)
names(ScaledinkSdStats) <- 0:9
#combine features
#assign to each label to correct ink feature
inkFeature <- 1:nrow(digit.dat)
for(i in 1:nrow(digit.dat)){
inkFeature[i] <- ScaledinkSdStats[digit.dat[i,]$label]
}
tmp <- data.frame(x=1:10, y=11:20)
#combine the digit with the ink feature
digit.ink <- cbind(digit.dat,inkFeature)
#Multinomial Logit
library(nnet)
train.index <- c(1:1000)
#initialise training and testData
#training set
digit.train <- digit.ink[train.index,]
digit.train$label = as.factor(digit.train$label)
digit.test <- digit.ink[-train.index,]
digit.test$label = as.factor(digit.test$label)
# fit multinomial logistic regression model
digit.multinom <- multinom(label ~ ., data = digit.train, maxit = 1000, MaxNWts = 100000000000000)
digit.multinom.pred <- predict(digit.multinom, digit.train[,-1],type="class")
# make confusion matrix: true label vs. predicted label
table(digit.train$label,digit.multinom.pred)
# predict class label on test data
digit.multinom.test.pred <- predict(digit.multinom, digit.test[,-1],type="class")
table(digit.test$label,digit.multinom.test.pred)
# make confusion matrix for predictions on test data
confmat <- table(digit.test$label,digit.multinom.test.pred)
# use it to compute accuracy on test data
sum(diag(confmat))/sum(confmat)
library(class)
attach(Smarket)
install.packages("Smarket")
?knn
digit.dat <- read.csv("dataset/mnist.csv", header = TRUE, sep = ",")
train.index <- c(1:1000)
#initialise training and testData
#training set
digit.train <- digit.ink[train.index,]
digit.train$label = as.factor(digit.train$label)
#test set
digit.test <- digit.ink[-train.index,]
digit.test$label = as.factor(digit.test$label)
digit.dat$label = as.factor(digit.dat$label)
#combine the digit with the ink feature
digit.ink <- cbind(digit.dat,inkFeature)
inkDensity <- apply(digit.dat[,-1],1,sum)
inkMeanStats <- tapply(inkDensity,digit.dat$label,mean)
inkSdStats <- tapply(inkDensity,digit.dat$label,sd)
#scale features
ScaledinkSdStats <- scale(inkSdStats)
names(ScaledinkSdStats) <- 0:9
#combine features
#assign to each label to correct ink feature
inkFeature <- 1:nrow(digit.dat)
for(i in 1:nrow(digit.dat)){
inkFeature[i] <- ScaledinkSdStats[digit.dat[i,]$label]
}
#combine the digit with the ink feature
digit.ink <- cbind(digit.dat,inkFeature)
digit.train <- digit.ink[train.index,]
digit.train$label = as.factor(digit.train$label)
#test set
digit.test <- digit.ink[-train.index,]
digit.test$label = as.factor(digit.test$label)
knn.pred <- knn(digit.train[,-1],digit.test[,-1],digit.train$label)
x = matrix(rnorm(40),20,2)
library("e1071")
set.seed(10111)
svmfit <- svm(label ~., data=digit.train, kernal="linear",cost=10,scale=FALSE)
table <- table(knn.pred, digit.test$label)
table
confmat <- table( digit.test$label, knn.pred)
confmat
sum(diag(confmat))/sum(confmat)
svmfit <- svm(label ~., data=digit.train, kernal="linear",cost=10,scale=FALSE)
print(svmfit)
plot(svmfitdigit.test)
confmat <- table( digit.test$label, svmfit)
?svm
library(glmnet)
digit.lasso.cv <- cv.glmnet(as.matrix(digit.train[,-1]), digit.train$label,family="multinomial", type.measure="class")
plot(digit.lasso.cv
)
plot(digit.lasso.cv)
digit.lasso.cv
digit.lasso.cv.pred <- predict(digit.lasso.cv, as.matrix(digit.test[,-1]),type="class")
digits.lasso.cv.confmat <- table(digit.test$label,digit.lasso.cv.pred)
sum(diag(digits.lasso.cv.confmat))/sum(digit.lasso.cv.confmat)
# compute the accuracy on the test set
sum(diag(digit.lasso.cv.confmat))/sum(digit.lasso.cv.confmat)
digit.lasso.cv.confmat <- table(digit.test$label,digit.lasso.cv.pred)
sum(diag(digit.lasso.cv.confmat))/sum(digit.lasso.cv.confmat)
digit.lasso.cv.confmat
sum(diag(digit.lasso.cv.confmat))/sum(digit.lasso.cv.confmat)
plot(digit.lasso.cv)
knn.pred
digit.svm <- svm(digit.train[, -1],digit.train$label)
SelectVar <- data.frame(x=c(1,0),b=c(0,0),c=c(0,1))
SelectVar
SelectVar[, colSums(SelectVar != 0) > 0]
digit.dat <- digit.dat[, colSums(digit.dat != 0) > 0]
ncol(digit.dat)
digit.dat <- read.csv("dataset/mnist.csv", header = TRUE, sep = ",")
ncol(digit.dat)
#remove all zero colls
digit.dat <- digit.dat[, colSums(digit.dat != 0) > 0]
digit.dat$label = as.factor(digit.dat$label)
ncol(digit.dat)
digit.svm <- svm(digit.train[, -1],digit.train$label)
digit.train <- digit.ink[train.index,]
digit.train$label = as.factor(digit.train$label)
#test set
digit.test <- digit.ink[-train.index,]
digit.test$label = as.factor(digit.test$label)
digit.ink <- cbind(digit.dat,inkFeature)
#combine features
#assign to each label to correct ink feature
inkFeature <- 1:nrow(digit.dat)
for(i in 1:nrow(digit.dat)){
inkFeature[i] <- ScaledinkSdStats[digit.dat[i,]$label]
}
digit.dat <- read.csv("dataset/mnist.csv", header = TRUE, sep = ",")
#preprocessing the data
digit.dat$label = as.factor(digit.dat$label)
#remove all zero colls
digit.dat <- digit.dat[, colSums(digit.dat != 0) > 0]
#checking stats. the result shows that a lot of pixels remain white for ALL 42000 samples. They can be clearly eliminated.
summary(digit.dat)
inkDensity <- apply(digit.dat[,-1],1,sum)
inkMeanStats <- tapply(inkDensity,digit.dat$label,mean)
inkSdStats <- tapply(inkDensity,digit.dat$label,sd)
#scale features
ScaledinkSdStats <- scale(inkSdStats)
names(ScaledinkSdStats) <- 0:9
#combine features
#assign to each label to correct ink feature
inkFeature <- 1:nrow(digit.dat)
for(i in 1:nrow(digit.dat)){
inkFeature[i] <- ScaledinkSdStats[digit.dat[i,]$label]
}
#combine the digit with the ink feature
digit.ink <- cbind(digit.dat,inkFeature)
#Multinomial Logit
library(nnet)
train.index <- c(1:1000)
#initialise training and testData
#training set
digit.train <- digit.ink[train.index,]
digit.train$label = as.factor(digit.train$label)
#test set
digit.test <- digit.ink[-train.index,]
digit.test$label = as.factor(digit.test$label)
set.seed(10111)
# SVM with radial kernel and gamma=1/62 and cost=1 (default settings)
digit.svm <- svm(digit.train[, -1],digit.train$label)
sum(digit.train$pixel12)
sum(digit.train$pixel11)
sum(digit.train$pixel15)
sum(digit.train$pixel16)
sum(digit.train$pixel19)
sum(digit.train$pixel20)
sum(digit.train$pixe20)
sum(digit.train$pixel20)
sum(digit.train$pixel40)
digit.dat <- digit.dat[, colSums(digit.dat != 0) > 0]
sum(digit.dat$pixel12)
digit.train <- digit.dat[train.index,]
digit.train$label = as.factor(digit.train$label)
#test set
digit.test <- digit.ink[-train.index,]
digit.test$label = as.factor(digit.test$label)
digit.svm <- svm(digit.train[, -1],digit.train$label)
sum(digit.train$pixel12)
sum(digit.dat$pixel12)
View(digit.train)
View(digit.dat)
ncol(digit.dat)
digit.train <- digit.dat[train.index,]
#remove all zero colls
digit.train <- digit.train[, colSums(digit.train != 0) > 0]
digit.svm <- svm(digit.train[, -1],digit.train$label)
svm.pred <- predict(optdigits.svm, digit.train$label)
svm.pred <- predict(digit.svm, digit.train$label)
length(digit.svm)
length(digit.train$label)
# make predictions on test set
svm.pred <- predict(digit.svm, digit.train$label)
svm.pred <- predict(digit.svm, digit.train$label)
svm.pred <- predict(optdigits.svm, digit.train[, -1])
svm.pred <- predict(digit.svm, digit.train[, -1])
svm.pred <- predict(digit.svm, digit.test[, -1])
digit.train <- digit.dat[train.index,]
digit.train$label = as.factor(digit.train$label)
digit.test <- digit.dat[-train.index,]
digit.test$label = as.factor(digit.test$label)
digit.svm <- svm(digit.train[, -1],digit.train$label)
svm.pred <- predict(digit.svm, digit.test[, -1])
table(digit.test$label,svm.pred)
sum(diag(table(digit.test$label,svm.pred)))/nrow(digit.test)
digit.svm <- svm(digit.train[, -1],digit.train$label)
svm.pred <- predict(digit.svm, digit.test[, -1])
svm.pred
table(digit.test$label,svm.pred)
digit.test <- digit.dat[-train.index,]
digit.test$label = as.factor(digit.test$label)
svm.pred <- predict(digit.svm, digit.test[, -1])
table(digit.test$label,svm.pred)
#k-nearest neigbhours
library(class)
#training set
digit.train <- digit.ink[train.index,]
digit.train$label = as.factor(digit.train$label)
#test set
digit.test <- digit.ink[-train.index,colnames(digit.train)]
digit.test$label = as.factor(digit.test$label)
knn.pred <- knn.cv(digit.train[,-1],digit.test[,-1],digit.train$label, k = 1)
length(digit.train)
length(digit.train$label)
nrow(digit.train[,-1])
knn.pred <- knn.cv(digit.train[,-1],digit.test[,-1],digit.train$label, k = 1)
#test set
digit.test <- digit.ink[-train.index,colnames(digit.train)]
digit.test$label = as.factor(digit.test$label)
knn.pred <- knn.cv(digit.train[,-1],digit.test[,-1],digit.train$label, k = 1)
knn(digit.train[,-1],digit.test[,-1],digit.train$label, k = 1)
confmat <- table( digit.test$label, knn.pred)
# use it to compute accuracy on test data
sum(diag(confmat))/sum(confmat)
confmat <- table( digit.test$label, knn.pred)
?knn
?knn.cv
knn.pred <- knn.cv(digit.train[,-1],digit.train$label, k = 1)
knn.pred
knn.pred
confmat <- table( digit.test$label, knn.pred)
knn.pred <- knn.cv(digit.train[,-1],digit.train$label, k = 1,prob= TRUE)
knn.pred
confmat <- table( digit.test$label, knn.pred)
knn.pred <- knn(digit.train[,-1],digit.test[,-1],digit.train$label, k = 1)
confmat <- table( digit.test$label, knn.pred)
sum(diag(confmat))/sum(confmat)
