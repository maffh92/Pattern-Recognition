#hPrice
hprice.dat <- read.table("Dataset/hprice.txt",header=TRUE)
hprice.dat
View(hprice.dat)
summary(hprice.dat)
hprice.lm1 <- lm(sale.price ~ lot.size + desire.loc,data=hprice.dat)
hprice.lm1
summary(hprice.lm1)
summary(hprice.dat)
# first you need to load the MASS package (you can also do this by selecting Load Package from the Packages menu).
library(MASS)
hprice.step <- stepAIC(hprice.lm2,scope=list(lower = ~1,upper = ~.^2),k=log(546))
# give a summary of the search process
hprice.step$anova
hprice.lm2 <- lm(sale.price ~ .,data=hprice.dat)
summary(hprice.lm2)
# first you need to load the MASS package (you can also do this by selecting Load Package from the Packages menu).
library(MASS)
hprice.step <- stepAIC(hprice.lm2,scope=list(lower = ~1,upper = ~.^2),k=log(546))
# give a summary of the search process
hprice.step$anova
summary(hprice.step)
x <- seq(0,1,len=10)
x <- seq(0,1,len=10)
x
t <- sin(2*pi*x)+rnorm(n=10,mean=0,sd=0.3)
t
plot(x,t,ylim=c(-2,2))
x.graph <- seq(0,1,by=0.001)
t.graph <- sin(2*pi*x.graph)
x.graph
t.graph
plot(x,t,ylim=c(-2,2))
lines(x.graph,t.graph,lwd=2,col=4)
install.packages("ridge")
install.packages("ridge", lib="S:/patrec/Rpackages/")
x.graph <- seq(0,1,by=0.001)
t.graph <- sin(2*pi*x.graph)
# plot the training set. You might want to adjust the value of ylim, depending on drawn data points and fitted functions
plot(x,t,ylim=c(-2,2))
# plot the "true" function
lines(x.graph,t.graph,lwd=2,col=4)
lm(t~x+I(x^2)+I(x^3),data.frame(t=t,x=x))
predict(poly.3,data.frame(x=x.graph))
# fit a 3rd order polynomial to the data
poly.3 <- lm(t~x+I(x^2)+I(x^3),data.frame(t=t,x=x))
predict(poly.3,data.frame(x=x.graph))
lines(x.graph,y.poly.3.graph,lwd=2,col=2)
lines(x.graph,y.poly.3.graph,lwd=2,col=2)
y.poly.3.graph <- predict(poly.3,data.frame(x=x.graph))
# plot the fitted function
lines(x.graph,y.poly.3.graph,lwd=2,col=2)
poly.9 <- lm(t~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8)+I(x^9),data.frame(t=t,x=x))
summary(poly.9)
t.testset <- sin(2*pi*x.graph)+rnorm(n=1001,mean=0,sd=0.3)
error.poly.3 <- sum((y.poly.3.graph-t.testset)^2)
error.poly.9 <- sum((y.poly.9.graph-t.testset)^2)
error.poly.3 <- sum((y.poly.3.graph-t.testset)^2)
error.poly.3
y.poly.9.graph <- predict(poly.9,data.frame(x=x.graph))
lines(x.graph,y.poly.9.graph,lwd=2,col=6)
#loading the data
digit.dat <- read.csv("dataset/mnist.csv", header = TRUE, sep = ",")
#preprocessing the data
digit.dat$label = as.factor(digit.dat$label)
#checking stats. the result shows that a lot of pixels remain white for ALL 42000 samples. They can be clearly eliminated.
summary(digit.dat)
inkDensity <- apply(digit.dat[,-1],1,sum)
inkMeanStats <- tapply(inkDensity,digit.dat$label,mean)
inkSdStats <- tapply(inkDensity,digit.dat$label,sd)
#scale features
ScaledinkSdStats <- scale(inkSdStats)
names(ScaledinkSdStats) <- 0:9
#combine features
#assign to each label to correct ink feature
inkFeature <- 1:nrow(digit.dat)
for(i in 1:nrow(digit.dat)){
inkFeature[i] <- ScaledinkSdStats[digit.dat[i,]$label]
}
tmp <- data.frame(x=1:10, y=11:20)
#combine the digit with the ink feature
digit.ink <- cbind(digit.dat,inkFeature)
#Multinomial Logit
library(nnet)
train.index <- c(1:1000)
#initialise training and testData
#training set
digit.train <- digit.ink[train.index,]
digit.train$label = as.factor(digit.train$label)
digit.test <- digit.ink[-train.index,]
digit.test$label = as.factor(digit.test$label)
# fit multinomial logistic regression model
digit.multinom <- multinom(label ~ ., data = digit.train, maxit = 1000, MaxNWts = 100000000000000)
digit.multinom.pred <- predict(digit.multinom, digit.train[,-1],type="class")
# make confusion matrix: true label vs. predicted label
table(digit.train$label,digit.multinom.pred)
# predict class label on test data
digit.multinom.test.pred <- predict(digit.multinom, digit.test[,-1],type="class")
table(digit.test$label,digit.multinom.test.pred)
# make confusion matrix for predictions on test data
confmat <- table(digit.test$label,digit.multinom.test.pred)
# use it to compute accuracy on test data
sum(diag(confmat))/sum(confmat)
library(class)
attach(Smarket)
install.packages("Smarket")
?knn
digit.dat <- read.csv("dataset/mnist.csv", header = TRUE, sep = ",")
train.index <- c(1:1000)
#initialise training and testData
#training set
digit.train <- digit.ink[train.index,]
digit.train$label = as.factor(digit.train$label)
#test set
digit.test <- digit.ink[-train.index,]
digit.test$label = as.factor(digit.test$label)
digit.dat$label = as.factor(digit.dat$label)
#combine the digit with the ink feature
digit.ink <- cbind(digit.dat,inkFeature)
inkDensity <- apply(digit.dat[,-1],1,sum)
inkMeanStats <- tapply(inkDensity,digit.dat$label,mean)
inkSdStats <- tapply(inkDensity,digit.dat$label,sd)
#scale features
ScaledinkSdStats <- scale(inkSdStats)
names(ScaledinkSdStats) <- 0:9
#combine features
#assign to each label to correct ink feature
inkFeature <- 1:nrow(digit.dat)
for(i in 1:nrow(digit.dat)){
inkFeature[i] <- ScaledinkSdStats[digit.dat[i,]$label]
}
#combine the digit with the ink feature
digit.ink <- cbind(digit.dat,inkFeature)
digit.train <- digit.ink[train.index,]
digit.train$label = as.factor(digit.train$label)
#test set
digit.test <- digit.ink[-train.index,]
digit.test$label = as.factor(digit.test$label)
knn.pred <- knn(digit.train[,-1],digit.test[,-1],digit.train$label)
x = matrix(rnorm(40),20,2)
library("e1071")
set.seed(10111)
svmfit <- svm(label ~., data=digit.train, kernal="linear",cost=10,scale=FALSE)
table <- table(knn.pred, digit.test$label)
table
confmat <- table( digit.test$label, knn.pred)
confmat
sum(diag(confmat))/sum(confmat)
svmfit <- svm(label ~., data=digit.train, kernal="linear",cost=10,scale=FALSE)
print(svmfit)
plot(svmfitdigit.test)
confmat <- table( digit.test$label, svmfit)
?svm
library(glmnet)
digit.lasso.cv <- cv.glmnet(as.matrix(digit.train[,-1]), digit.train$label,family="multinomial", type.measure="class")
plot(digit.lasso.cv
)
plot(digit.lasso.cv)
digit.lasso.cv
digit.lasso.cv.pred <- predict(digit.lasso.cv, as.matrix(digit.test[,-1]),type="class")
digits.lasso.cv.confmat <- table(digit.test$label,digit.lasso.cv.pred)
sum(diag(digits.lasso.cv.confmat))/sum(digit.lasso.cv.confmat)
# compute the accuracy on the test set
sum(diag(digit.lasso.cv.confmat))/sum(digit.lasso.cv.confmat)
digit.lasso.cv.confmat <- table(digit.test$label,digit.lasso.cv.pred)
sum(diag(digit.lasso.cv.confmat))/sum(digit.lasso.cv.confmat)
digit.lasso.cv.confmat
sum(diag(digit.lasso.cv.confmat))/sum(digit.lasso.cv.confmat)
plot(digit.lasso.cv)
knn.pred
digit.svm <- svm(digit.train[, -1],digit.train$label)
SelectVar <- data.frame(x=c(1,0),b=c(0,0),c=c(0,1))
SelectVar
SelectVar[, colSums(SelectVar != 0) > 0]
digit.dat <- digit.dat[, colSums(digit.dat != 0) > 0]
ncol(digit.dat)
digit.dat <- read.csv("dataset/mnist.csv", header = TRUE, sep = ",")
ncol(digit.dat)
#remove all zero colls
digit.dat <- digit.dat[, colSums(digit.dat != 0) > 0]
digit.dat$label = as.factor(digit.dat$label)
ncol(digit.dat)
digit.svm <- svm(digit.train[, -1],digit.train$label)
digit.train <- digit.ink[train.index,]
digit.train$label = as.factor(digit.train$label)
#test set
digit.test <- digit.ink[-train.index,]
digit.test$label = as.factor(digit.test$label)
digit.ink <- cbind(digit.dat,inkFeature)
#combine features
#assign to each label to correct ink feature
inkFeature <- 1:nrow(digit.dat)
for(i in 1:nrow(digit.dat)){
inkFeature[i] <- ScaledinkSdStats[digit.dat[i,]$label]
}
digit.dat <- read.csv("dataset/mnist.csv", header = TRUE, sep = ",")
#preprocessing the data
digit.dat$label = as.factor(digit.dat$label)
#remove all zero colls
digit.dat <- digit.dat[, colSums(digit.dat != 0) > 0]
#checking stats. the result shows that a lot of pixels remain white for ALL 42000 samples. They can be clearly eliminated.
summary(digit.dat)
inkDensity <- apply(digit.dat[,-1],1,sum)
inkMeanStats <- tapply(inkDensity,digit.dat$label,mean)
inkSdStats <- tapply(inkDensity,digit.dat$label,sd)
#scale features
ScaledinkSdStats <- scale(inkSdStats)
names(ScaledinkSdStats) <- 0:9
#combine features
#assign to each label to correct ink feature
inkFeature <- 1:nrow(digit.dat)
for(i in 1:nrow(digit.dat)){
inkFeature[i] <- ScaledinkSdStats[digit.dat[i,]$label]
}
#combine the digit with the ink feature
digit.ink <- cbind(digit.dat,inkFeature)
#Multinomial Logit
library(nnet)
train.index <- c(1:1000)
#initialise training and testData
#training set
digit.train <- digit.ink[train.index,]
digit.train$label = as.factor(digit.train$label)
#test set
digit.test <- digit.ink[-train.index,]
digit.test$label = as.factor(digit.test$label)
set.seed(10111)
# SVM with radial kernel and gamma=1/62 and cost=1 (default settings)
digit.svm <- svm(digit.train[, -1],digit.train$label)
sum(digit.train$pixel12)
sum(digit.train$pixel11)
sum(digit.train$pixel15)
sum(digit.train$pixel16)
sum(digit.train$pixel19)
sum(digit.train$pixel20)
sum(digit.train$pixe20)
sum(digit.train$pixel20)
sum(digit.train$pixel40)
digit.dat <- digit.dat[, colSums(digit.dat != 0) > 0]
sum(digit.dat$pixel12)
digit.train <- digit.dat[train.index,]
digit.train$label = as.factor(digit.train$label)
#test set
digit.test <- digit.ink[-train.index,]
digit.test$label = as.factor(digit.test$label)
digit.svm <- svm(digit.train[, -1],digit.train$label)
sum(digit.train$pixel12)
sum(digit.dat$pixel12)
View(digit.train)
View(digit.dat)
ncol(digit.dat)
digit.train <- digit.dat[train.index,]
#remove all zero colls
digit.train <- digit.train[, colSums(digit.train != 0) > 0]
digit.svm <- svm(digit.train[, -1],digit.train$label)
svm.pred <- predict(optdigits.svm, digit.train$label)
svm.pred <- predict(digit.svm, digit.train$label)
length(digit.svm)
length(digit.train$label)
# make predictions on test set
svm.pred <- predict(digit.svm, digit.train$label)
svm.pred <- predict(digit.svm, digit.train$label)
svm.pred <- predict(optdigits.svm, digit.train[, -1])
svm.pred <- predict(digit.svm, digit.train[, -1])
svm.pred <- predict(digit.svm, digit.test[, -1])
digit.train <- digit.dat[train.index,]
digit.train$label = as.factor(digit.train$label)
digit.test <- digit.dat[-train.index,]
digit.test$label = as.factor(digit.test$label)
digit.svm <- svm(digit.train[, -1],digit.train$label)
svm.pred <- predict(digit.svm, digit.test[, -1])
table(digit.test$label,svm.pred)
sum(diag(table(digit.test$label,svm.pred)))/nrow(digit.test)
digit.svm <- svm(digit.train[, -1],digit.train$label)
svm.pred <- predict(digit.svm, digit.test[, -1])
svm.pred
table(digit.test$label,svm.pred)
digit.test <- digit.dat[-train.index,]
digit.test$label = as.factor(digit.test$label)
svm.pred <- predict(digit.svm, digit.test[, -1])
table(digit.test$label,svm.pred)
library(class)
knn.pred <- knn.cv(digit.train[,-1],digit.test[,-1],digit.train$label, k = 1)
?knn.cv
knn.pred.cv <- knn.cv(digit.train[,-1],digit.train$label, k = 1)
knn.pred.cv <- knn.cv(digit.train[,-1],digit.train$label, k = 1:10)
knn.pred.cv <- knn.cv(digit.train[,-1],digit.train$label, k = 1)
knn.pred.cv
confmat <- table( digit.test$label, knn.pred)
confmat <- table( digit.test$label, knn.pred.cv)
knn.pred.cv <- knn.cv(digit.train[,-1],digit.train$label, k = 10)
knn.pred.cv <- knn.cv(digit.train[,-1],digit.train$label, nfolds = 10)
knn.pred.cv <- knn.cv(digit.train[,-1],digit.train$label, 10)
attributes(.Last.value)
knn.pred.cv$levels
knn.pred.cv
knn(digit.train[,-1],digit.test[,-1],digit.train$label, k = 1)
knn.cv
library(caret)
set.seed(1)
idx <- createFolds(digit.train , k=10)
View(idx)
idx
sapply(idx, length)
split(1:1000, ceiling(seq_along(1:1000)/100))
split(1:1000, ceiling(seq_along(1:1000)/50))
split(1:1000, ceiling(seq_along(1:1000)/100))
?diag
knn.pred <- knn(digit.train[,-1],digit.test[,-1],digit.train$label, k = 1)
knn.pred.cv <- knn.cv(digit.train[,-1],digit.train$label, k = 1)
confmat <- table( digit.test$label, knn.pred.cv)
confmat <- table( digit.test$label, knn.pred)
confmat
confmat + 1
confmat
confmat + confmat
matthewmatthew <- matthewmatthew + 1
matthew <- data.frame()
matthew <- data.frame(0:9)
matthew
matthew <- data.frame()
colnames(matthew) <- 0:10
?rep
matthew <- data.frame()
for(i in 0:10){
matthew[,i] <- rep(0,10)
}
matthew <- data.frame(0==c(),1=c(),2=c(),3=c(),4=c(),5=c(),6=c(),7=c(),8=c(),9=c())
matthew <- data.frame(0=c(),1=c(),2=c(),3=c(),4=c(),5=c(),6=c(),7=c(),8=c(),9=c())
matthew <- data.frame(0=rep(0,10),1=rep(0,10),2=rep(0,10),3=rep(0,10),4=rep(0,10),5=rep(0,10),6=rep(0,10),7=rep(0,10),8=rep(0,10),9=rep(0,10))
matthew <- data.frame("0"=rep(0,10),"1"=rep(0,10),"2"=rep(0,10),"3"=rep(0,10),"4"=rep(0,10),"5"=rep(0,10),"6"=rep(0,10),"7"=rep(0,10),"8"=rep(0,10),"9"=rep(0,10))
matthew
matthew <- data.frame("0"=rep(0,10),"1"=rep(0,10),"2"=rep(0,10),"3"=rep(0,10),"4"=rep(0,10),"5"=rep(0,10),"6"=rep(0,10),"7"=rep(0,10),"8"=rep(0,10),"9"=rep(0,10))
colnames(matthew) <- 0:9
matthew
# listToVectorExpr is used to exclude one group of a list.
# The data parameter is a list of 10 groups and except is the number of the group to be excluded from tr. set
listToVectorExcept <- function(data, except){
value <- c()
for(i in 1:NROW(data)){
if(i != except){
value <- append(value, data[[i]])
}
}
return(value)
}
#ten.fold.split first divides the groups in equal divided groups of 20. The data is already random, so we do not make it random here.
# Then it will go over all of these groups and classifies them to the other 180 elements and sum the total errors.
ten.fold.split <- function(data,k){ # returns total error for 10-fold validation
rowsToDivide <- 1 : nrow(data)
#split the data set into equal divided groups
equalDividedGroups <- split(rowsToDivide, ceiling(seq_along(rowsToDivide)/100)) #
confmat <- data.frame("0"=rep(0,10),"1"=rep(0,10),"2"=rep(0,10),"3"=rep(0,10),"4"=rep(0,10),"5"=rep(0,10),"6"=rep(0,10),"7"=rep(0,10),"8"=rep(0,10),"9"=rep(0,10))
colnames(confmat) <- 0:9
for(i in 1:NROW(equalDividedGroups)){
#split the training set and testset
trainingSet <- data[listToVectorExcept(equalDividedGroups,i),]
testSet <- data[equalDividedGroups[[i]],]
knn.pred <- knn(trainingSet[,-1],testSet[,-1],trainingSet[,1], k = k)
confmat <- confmat + table(testSet[,1], knn.pred)
# use it to compute accuracy on test data
}
return(sum(diag(confmat))/sum(confmat))
}
#run.analyse is used as the main function to run the analyse
#classifyInformation is used to store the parameters with the corresponding number of errors.
run.analyse <- function(data,k){
#get a random of size 200, first attribute of the data variable is the nr
classifyInformation = data.frame(k = c(), accuracy = c())
for(i in 1 : k){ # nmin and minleaf may have different range
value <- ten.fold.split(trainingSet,i) # total error for 10-fold validation
classifyInformation = rbind(classifyInformation, c(i, value))
}
names(classifyInformation) <- c("k","accuracy")
return(classifyInformation)
}
neighboursInformation <- run.analyse(digit.dat[train.index,],1)
run.analyse <- function(data,k){
#get a random of size 200, first attribute of the data variable is the nr
classifyInformation = data.frame(k = c(), accuracy = c())
for(i in 1 : k){ # nmin and minleaf may have different range
value <- ten.fold.split(data,i) # total error for 10-fold validation
classifyInformation = rbind(classifyInformation, c(i, value))
}
names(classifyInformation) <- c("k","accuracy")
return(classifyInformation)
}
neighboursInformation <- run.analyse(digit.dat[train.index,],1)
ten.fold.split(digit.dat[train.index,],1)
ten.fold.split <- function(data,k){ # returns total error for 10-fold validation
rowsToDivide <- 1 : nrow(data)
#split the data set into equal divided groups
equalDividedGroups <- split(rowsToDivide, ceiling(seq_along(rowsToDivide)/100)) #
confmat <- data.frame("0"=rep(0,10),"1"=rep(0,10),"2"=rep(0,10),"3"=rep(0,10),"4"=rep(0,10),"5"=rep(0,10),"6"=rep(0,10),"7"=rep(0,10),"8"=rep(0,10),"9"=rep(0,10))
colnames(confmat) <- 0:9
for(i in 1:NROW(equalDividedGroups)){
#split the training set and testset
trainingSet <- data[listToVectorExcept(equalDividedGroups,i),]
testSet <- data[equalDividedGroups[[i]],]
knn.pred <- knn(trainingSet[,-1],testSet[,-1],trainingSet[,1], k = k)
confmat <- confmat + table(testSet[,1], knn.pred)
# use it to compute accuracy on test data
}
confmat <- lapply(confmat, as.numeric)
return(sum(diag(confmat))/sum(confmat))
}
ten.fold.split(digit.dat[train.index,],1)
ten.fold.split <- function(data,k){ # returns total error for 10-fold validation
rowsToDivide <- 1 : nrow(data)
#split the data set into equal divided groups
equalDividedGroups <- split(rowsToDivide, ceiling(seq_along(rowsToDivide)/100)) #
confmat <- data.frame("0"=rep(0,10),"1"=rep(0,10),"2"=rep(0,10),"3"=rep(0,10),"4"=rep(0,10),"5"=rep(0,10),"6"=rep(0,10),"7"=rep(0,10),"8"=rep(0,10),"9"=rep(0,10))
colnames(confmat) <- 0:9
for(i in 1:NROW(equalDividedGroups)){
#split the training set and testset
trainingSet <- data[listToVectorExcept(equalDividedGroups,i),]
testSet <- data[equalDividedGroups[[i]],]
knn.pred <- knn(trainingSet[,-1],testSet[,-1],trainingSet[,1], k = k)
confmat <- lapply(confmat, as.numeric)
confmat <- confmat + table(testSet[,1], knn.pred)
# use it to compute accuracy on test data
}
return(sum(diag(confmat))/sum(confmat))
}
ten.fold.split(digit.dat[train.index,],1)
#ten.fold.split first divides the groups in equal divided groups of 20. The data is already random, so we do not make it random here.
# Then it will go over all of these groups and classifies them to the other 180 elements and sum the total errors.
ten.fold.split <- function(data,k){ # returns total error for 10-fold validation
rowsToDivide <- 1 : nrow(data)
#split the data set into equal divided groups
equalDividedGroups <- split(rowsToDivide, ceiling(seq_along(rowsToDivide)/100)) #
confmat <- data.frame("0"=rep(0,10),"1"=rep(0,10),"2"=rep(0,10),"3"=rep(0,10),"4"=rep(0,10),"5"=rep(0,10),"6"=rep(0,10),"7"=rep(0,10),"8"=rep(0,10),"9"=rep(0,10))
colnames(confmat) <- 0:9
for(i in 1:NROW(equalDividedGroups)){
#split the training set and testset
trainingSet <- data[listToVectorExcept(equalDividedGroups,i),]
testSet <- data[equalDividedGroups[[i]],]
knn.pred <- knn(trainingSet[,-1],testSet[,-1],trainingSet[,1], k = k)
confmat <- confmat + table(testSet[,1], knn.pred)
# use it to compute accuracy on test data
}
return(confmat)
# return(sum(diag(confmat))/sum(confmat))
}
ten.fold.split(digit.dat[train.index,],1)
neighboursInformation <- run.analyse(digit.dat[train.index,],1)
b <- ten.fold.split(digit.dat[train.index,],1)
sum(diag(b))/sum(b)
sum(b)
diag(b)
b <- as.numeric(unlist(ab)
;
b <- as.numeric(unlist(b))
diag(b)
b <- ten.fold.split(digit.dat[train.index,],1)
b
b <- as.data.frame(b)
diag(b)
b
b <- lapply(a, as.numeric)
b <- lapply(b, as.numeric)
diag(b)
b[1,1]
as.data.frame(b)[1,1]
b <- as.data.frame(b)
b[1,1]
b
diag(b)
b <- apply(b, c(1,2) ,as.numeric)
diag(b)
b
b <- apply(as.data.frame(b), c(1,2) ,as.numeric)
b <- ten.fold.split(digit.dat[train.index,],1)
ba <- apply(as.data.frame(b), c(1,2) ,as.numeric)
b
ba
ba==b
b + 1
ba==b
b <- b + 1
ba==b
#ten.fold.split first divides the groups in equal divided groups of 20. The data is already random, so we do not make it random here.
# Then it will go over all of these groups and classifies them to the other 180 elements and sum the total errors.
ten.fold.split <- function(data,k){ # returns total error for 10-fold validation
rowsToDivide <- 1 : nrow(data)
#split the data set into equal divided groups
equalDividedGroups <- split(rowsToDivide, ceiling(seq_along(rowsToDivide)/100)) #
confmat <- data.frame("0"=rep(0,10),"1"=rep(0,10),"2"=rep(0,10),"3"=rep(0,10),"4"=rep(0,10),"5"=rep(0,10),"6"=rep(0,10),"7"=rep(0,10),"8"=rep(0,10),"9"=rep(0,10))
colnames(confmat) <- 0:9
for(i in 1:NROW(equalDividedGroups)){
#split the training set and testset
trainingSet <- data[listToVectorExcept(equalDividedGroups,i),]
testSet <- data[equalDividedGroups[[i]],]
knn.pred <- knn(trainingSet[,-1],testSet[,-1],trainingSet[,1], k = k)
confmat <- confmat + table(testSet[,1], knn.pred)
# use it to compute accuracy on test data
}
#need to convert it to numeric, for some reason it is not numeric anymore
confmat <- apply(as.data.frame(confmat), c(1,2) ,as.numeric)
return(sum(diag(confmat))/sum(confmat))
}
ten.fold.split(digit.dat[train.index,],1)
neighboursInformation <- run.analyse(digit.dat[train.index,],1)
neighboursInformation
neighboursInformation <- run.analyse(digit.dat[train.index,],10)
neighboursInformation
neighboursInformation[min(neighboursInformation),]
sample(c(1:nrow(digit.dat)),1000)
length(sample(c(1:nrow(digit.dat)),1000))
